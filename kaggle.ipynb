{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!mkdir Data\n",
    "!mkdir Models\n",
    "!mkdir Media\n",
    "!mkdir Media/SSL\n",
    "!mkdir Models/SSL"
   ],
   "id": "8c48da27c5567246"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# network",
   "id": "d2f4126e2593ace"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes, size):\n",
    "\n",
    "        # list of the conv layers parameters\n",
    "        convLayerNumber = 7\n",
    "        kernels = [11, 7, 7, 5, 5, 3, 3]\n",
    "        paddings = [1, 1, 1, 1, 1, 1, 1]\n",
    "        poolingsStride = [2, 0, 2, 0, 2, 0, 2]\n",
    "        poolingsKernels = [2, 0, 2, 0, 2, 0, 2]\n",
    "        filters = [8, 16, 16, 32, 64, 64, 128]\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "        # Define the feature extraction part of the network\n",
    "        self.conv1 = nn.Conv2d(3, filters[0], kernel_size=kernels[0], padding=paddings[0])\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=poolingsKernels[0], stride=poolingsStride[0])\n",
    "\n",
    "        self.conv2 = nn.Conv2d(filters[0], filters[1], kernel_size=kernels[1], padding=paddings[1])\n",
    "        self.conv3 = nn.Conv2d(filters[1], filters[2], kernel_size=kernels[2], padding=paddings[2])\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=poolingsKernels[2], stride=poolingsStride[2])\n",
    "\n",
    "        self.conv4 = nn.Conv2d(filters[2], filters[3], kernel_size=kernels[3], padding=paddings[3])\n",
    "        self.conv5 = nn.Conv2d(filters[3], filters[4], kernel_size=kernels[4], padding=paddings[4])\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=poolingsKernels[4], stride=poolingsStride[4])\n",
    "\n",
    "        self.conv6 = nn.Conv2d(filters[4], filters[5], kernel_size=kernels[5], padding=paddings[5])\n",
    "        self.conv7 = nn.Conv2d(filters[5], filters[6], kernel_size=kernels[6], padding=paddings[6])\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=poolingsKernels[6], stride=poolingsStride[6])\n",
    "\n",
    "        # now we want to calculate the final dimension of all the conv layers\n",
    "        for i in range(convLayerNumber):\n",
    "            size = (size - kernels[i] + 2 * paddings[i]) / 1 + 1\n",
    "            if (poolingsKernels[i] != 0):\n",
    "                size = int((size - poolingsKernels[i]) / poolingsStride[i] + 1)\n",
    "\n",
    "        fc1_input_size = filters[6] * size * size\n",
    "\n",
    "        print(\"First layer size: \", fc1_input_size)\n",
    "\n",
    "        # Define the classification part of the network\n",
    "        self.fc1 = nn.Linear(fc1_input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "        # self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        # self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv4(x))\n",
    "        x = F.leaky_relu(self.conv5(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv6(x))\n",
    "        x = F.leaky_relu(self.conv7(x))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Classification\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def trainStep(self, x, y, optimizer, criterion):\n",
    "        optimizer.zero_grad()\n",
    "        out = self.forward(x)\n",
    "\n",
    "        # create a tensor for each class\n",
    "        target = torch.zeros((y.size(0), 251))\n",
    "        target[range(y.size(0)), y] = 1\n",
    "\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n"
   ],
   "id": "51e887524c791bb3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# train",
   "id": "45dedcb798ba4373"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torchvision.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "def main(loadPreTrained: bool):\n",
    "    train_on_gpu = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if train_on_gpu else \"cpu\")\n",
    "    print(\"running on: \", device)\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        mps_device = torch.device(\"mps\")\n",
    "        x = torch.ones(1, device=mps_device)\n",
    "        print(x)\n",
    "    else:\n",
    "        print(\"MPS device not found.\")\n",
    "\n",
    "    size = 128\n",
    "    mean = [0.6388, 0.5446, 0.4452]\n",
    "    std = [0.2252, 0.2437, 0.2661]\n",
    "    net = Net(num_classes=251, size=size)\n",
    "\n",
    "    # load a .pth file into the model in order to start from a pre-trained model\n",
    "    if loadPreTrained:\n",
    "        net.load_state_dict(torch.load('Models/128- model_2.pth'))\n",
    "        net.eval()\n",
    "\n",
    "    net.to(device)\n",
    "\n",
    "    lossOvertime = []\n",
    "    accuracyOvertime = []\n",
    "\n",
    "    # Define data transformations pipeline\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Resize((size, size), antialias=True),\n",
    "        torchvision.transforms.Normalize(mean=mean, std=std),\n",
    "        \n",
    "    ])\n",
    "\n",
    "    trainSet = torchvision.datasets.ImageFolder(root='/kaggle/input/supervised/processedData/processed_train_set', transform=transforms)\n",
    "    testSet = torchvision.datasets.ImageFolder(root='/kaggle/input/supervised/processedData/processed_test_set', transform=transforms)\n",
    "    valSet = torchvision.datasets.ImageFolder(root='/kaggle/input/supervised/processedData/processed_val_set', transform=transforms)\n",
    "\n",
    "    trainLoader = DataLoader(trainSet, batch_size=64, shuffle=True, num_workers=4)\n",
    "    testLoader = DataLoader(testSet, batch_size=64, shuffle=True, num_workers=4)\n",
    "    valLoader = DataLoader(valSet, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(ssl_model.parameters(), lr=0.001)\n",
    "\n",
    "    # define a learning rate scheduler\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0.0001)\n",
    "\n",
    "    epochs = 10\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in tqdm(enumerate(trainLoader, 0), total=len(trainLoader)):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            loss = net.trainStep(inputs, labels, optimizer, criterion)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        lossOvertime.append(round(running_loss/len(trainLoader), 2))\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        net.eval()\n",
    "\n",
    "        # check accuracy on val set\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(valLoader, total=len(valLoader)):\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = net(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        accuracy = round(accuracy, 2)\n",
    "        accuracyOvertime.append(accuracy)\n",
    "        print(f\"Epoch {epoch + 1}, loss: {round(running_loss/len(trainLoader), 2)}, accuracy: {accuracy}\")\n",
    "\n",
    "        net.train()\n",
    "\n",
    "        title = \"\"\n",
    "        if loadPreTrained:\n",
    "            title = \"-Pre-trained model\"\n",
    "\n",
    "        # save model\n",
    "        torch.save(net.state_dict(), f\"Models/{size}-model_{epoch}{title}_corrected.pth\")\n",
    "\n",
    "    print('Finished Training')\n",
    "    print(lossOvertime)\n",
    "    print(accuracyOvertime)\n",
    "\n",
    "    # plot loss and accuracy in separate graphs\n",
    "    plt.plot(lossOvertime)\n",
    "    plt.savefig('Media/loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(accuracyOvertime)\n",
    "    plt.savefig('Media/accuracy.png')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Starting testing\")\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    # validate the model on the test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testLoader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation accuracy: {accuracy}\")\n",
    "\n",
    "    # calculate the F1 score\n",
    "    print(\"Starting F1 score calculation\")\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data in valLoader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_true += labels.tolist()\n",
    "            y_pred += predicted.tolist()\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"F1 score: {f1}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    loadPreTrained = False\n",
    "\n",
    "    main(loadPreTrained)"
   ],
   "id": "db74b5769c348be5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SSL",
   "id": "67e7d5e4a019d1cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "mean = [0.6388, 0.5446, 0.4452]\n",
    "std = [0.2252, 0.2437, 0.2661]\n",
    "\n",
    "class JigsawPuzzleDataset(Dataset):\n",
    "    def __init__(self, dataset, grid_size=2):\n",
    "        self.dataset = dataset\n",
    "        self.grid_size = grid_size\n",
    "        self.permutations = self._generate_permutations(grid_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, _ = self.dataset[index]\n",
    "        image_pieces, correct_order = self._divide_image(image)\n",
    "        shuffled_pieces, shuffled_order = self._shuffle_pieces(image_pieces, correct_order)\n",
    "\n",
    "        # Create a new blank image of the correct size\n",
    "        reconstructed_image = torch.zeros_like(image)\n",
    "\n",
    "        piece_w, piece_h = image.shape[1] // self.grid_size, image.shape[2] // self.grid_size\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                # Calculate the position of the piece in the reconstructed image\n",
    "                pos = (j * piece_w, i * piece_h)\n",
    "                # Paste the piece at the correct position\n",
    "                reconstructed_image[:, pos[1]:pos[1]+piece_h, pos[0]:pos[0]+piece_w] = shuffled_pieces[i * self.grid_size + j]\n",
    "\n",
    "        # One-hot encode the permutation index\n",
    "        permutation_index = self.permutations.index(tuple(shuffled_order))\n",
    "        label = torch.zeros(len(self.permutations), dtype=torch.float)\n",
    "        label[permutation_index] = 1.0\n",
    "\n",
    "        return reconstructed_image, label\n",
    "\n",
    "    def _divide_image(self, image):\n",
    "        pieces = []\n",
    "\n",
    "        piece_w, piece_h = image.shape[1] // self.grid_size, image.shape[2] // self.grid_size\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                piece = image[:, i*piece_h:(i+1)*piece_h, j*piece_w:(j+1)*piece_w]\n",
    "                pieces.append(piece)\n",
    "\n",
    "        correct_order = list(range(self.grid_size ** 2))\n",
    "        return pieces, correct_order\n",
    "\n",
    "    def _shuffle_pieces(self, pieces, correct_order):\n",
    "        shuffled_order = random.choice(self.permutations)\n",
    "        shuffled_pieces = [pieces[i] for i in shuffled_order]\n",
    "        return shuffled_pieces, shuffled_order\n",
    "\n",
    "    def _generate_permutations(self, grid_size):\n",
    "        indices = list(range(grid_size ** 2))\n",
    "        permutations = list(itertools.permutations(indices))\n",
    "        return permutations\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes, size):\n",
    "\n",
    "        # list of the conv layers parameters\n",
    "        convLayerNumber = 7\n",
    "        kernels = [11, 7, 7, 5, 5, 3, 3]\n",
    "        paddings = [1, 1, 1, 1, 1, 1, 1]\n",
    "        poolingsStride = [2, 0, 2, 0, 2, 0, 2]\n",
    "        poolingsKernels = [2, 0, 2, 0, 2, 0, 2]\n",
    "        filters = [8, 16, 16, 32, 64, 64, 128]\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "        # Define the feature extraction part of the network\n",
    "        self.conv1 = nn.Conv2d(3, filters[0], kernel_size=kernels[0], padding=paddings[0])\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=poolingsKernels[0], stride=poolingsStride[0])\n",
    "\n",
    "        self.conv2 = nn.Conv2d(filters[0], filters[1], kernel_size=kernels[1], padding=paddings[1])\n",
    "        self.conv3 = nn.Conv2d(filters[1], filters[2], kernel_size=kernels[2], padding=paddings[2])\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=poolingsKernels[2], stride=poolingsStride[2])\n",
    "\n",
    "        self.conv4 = nn.Conv2d(filters[2], filters[3], kernel_size=kernels[3], padding=paddings[3])\n",
    "        self.conv5 = nn.Conv2d(filters[3], filters[4], kernel_size=kernels[4], padding=paddings[4])\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=poolingsKernels[4], stride=poolingsStride[4])\n",
    "\n",
    "        self.conv6 = nn.Conv2d(filters[4], filters[5], kernel_size=kernels[5], padding=paddings[5])\n",
    "        self.conv7 = nn.Conv2d(filters[5], filters[6], kernel_size=kernels[6], padding=paddings[6])\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=poolingsKernels[6], stride=poolingsStride[6])\n",
    "\n",
    "        # now we want to calculate the final dimension of all the conv layers\n",
    "        for i in range(convLayerNumber):\n",
    "            size = (size - kernels[i] + 2 * paddings[i]) / 1 + 1\n",
    "            if (poolingsKernels[i] != 0):\n",
    "                size = int((size - poolingsKernels[i]) / poolingsStride[i] + 1)\n",
    "\n",
    "        fc1_input_size = filters[6] * size * size\n",
    "\n",
    "        print(\"First layer size: \", fc1_input_size)\n",
    "\n",
    "        # Define the classification part of the network\n",
    "        self.fc1 = nn.Linear(fc1_input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "class NetWithJigSawPrediction(Net):\n",
    "    def __init__(self, num_classes, size):\n",
    "        super(NetWithJigSawPrediction, self).__init__(num_classes, size)\n",
    "        self.rotation_fc = nn.Linear(self.fc1.in_features, 2)\n",
    "\n",
    "    def forward(self, x, predict_rotation=False):\n",
    "        # Feature extraction\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv4(x))\n",
    "        x = F.leaky_relu(self.conv5(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv6(x))\n",
    "        x = F.leaky_relu(self.conv7(x))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        if predict_rotation:\n",
    "            return self.rotation_fc(x)\n",
    "        else:\n",
    "            x = F.leaky_relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            return self.fc2(x)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to a standard size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "trainSet = datasets.ImageFolder(root='/kaggle/input/supervised/processedData/processed_train_set', transform=transform)\n",
    "testSet = datasets.ImageFolder(root='/kaggle/input/supervised/processedData/processed_test_set', transform=transform)\n",
    "valSet = datasets.ImageFolder(root='/kaggle/input/supervised/processedData/processed_val_set', transform=transform)\n",
    "rotation_dataset = JigsawPuzzleDataset(trainSet)\n",
    "rotation_datasetTest = JigsawPuzzleDataset(testSet)\n",
    "rotation_datasetVal = JigsawPuzzleDataset(valSet)\n",
    "\n",
    "batch_size = 64\n",
    "rotation_loader = DataLoader(rotation_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "rotation_loaderTest = DataLoader(rotation_datasetTest, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "rotation_loaderVal = DataLoader(rotation_datasetVal, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "print(\"Validation:\", len(rotation_loader.dataset))\n",
    "print(\"Training:\", len(rotation_loaderTest.dataset))\n",
    "print(\"Test:\", len(rotation_loaderVal.dataset))\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Running on {device}\")\n",
    "\n",
    "# Initialize the SSL model\n",
    "ssl_model = NetWithJigSawPrediction(num_classes=251, size=128)\n",
    "ssl_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ssl_model.parameters(), lr=0.001)\n",
    "\n",
    "# define a learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "lossOvertime = []\n",
    "accuracyOvertime = []\n",
    "\n",
    "ssl_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(rotation_loader, total=len(rotation_loader)):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ssl_model(images, predict_rotation=True)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    lossOvertime.append(round(running_loss / len(rotation_loader), 2))\n",
    "\n",
    "    # put the model in evaluation mode\n",
    "    ssl_model.eval()\n",
    "\n",
    "    # check accuracy on val set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(rotation_loaderVal, total=len(rotation_loaderVal)):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = ssl_model(images, predict_rotation=True)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    accuracy = round(accuracy, 2)\n",
    "    accuracyOvertime.append(accuracy)\n",
    "    print(f\"Epoch {epoch + 1}, loss: {round(running_loss / len(rotation_loader), 2)}, accuracy: {accuracy}\")\n",
    "\n",
    "    # put the model back in training mode\n",
    "    ssl_model.train()\n",
    "\n",
    "    # save the model after each epoch\n",
    "    torch.save(ssl_model.state_dict(), f\"ssl_model_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "print(\"Finished SSL Training\")\n",
    "print(lossOvertime)\n",
    "print(accuracyOvertime)\n",
    "\n",
    "# plot loss and accuracy in separate graphs\n",
    "plt.plot(lossOvertime)\n",
    "plt.savefig('Media/SSL/loss.png')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(accuracyOvertime)\n",
    "plt.savefig('Media/SSL/accuracy.png')\n",
    "plt.close()\n",
    "\n",
    "# start testing\n",
    "print(\"Starting testing\")\n",
    "\n",
    "# put the model in evaluation mode\n",
    "ssl_model.eval()\n",
    "\n",
    "# validate the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in rotation_loaderTest:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = ssl_model(images, predict_rotation=True)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Testings accuracy: {accuracy}\")\n",
    "\n",
    "# load the best model\n",
    "ssl_model.load_state_dict(torch.load(\"ssl_model_epoch_5.pth\"))\n",
    "\n",
    "classification_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "classification_dataset = datasets.ImageFolder(root=\"/kaggle/input/supervised/processedData/processed_train_set\",\n",
    "                                          transform=classification_transform)\n",
    "classification_datasetTest = datasets.ImageFolder(root=\"./kaggle/input/supervised/processedData/processed_test_set\",\n",
    "                                              transform=classification_transform)\n",
    "classification_datasetVal = datasets.ImageFolder(root=\"/kaggle/input/supervised/processedData/processed_val_set\",\n",
    "                                             transform=classification_transform)\n",
    "\n",
    "classification_loader = DataLoader(classification_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "classification_loaderTest = DataLoader(classification_datasetTest, batch_size=batch_size, shuffle=True,\n",
    "                                   num_workers=4)\n",
    "classification_loaderVal = DataLoader(classification_datasetVal, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Fine-tune the SSL model for classification\n",
    "ssl_model.fc2 = nn.Linear(ssl_model.fc2.in_features, 251)  # Update the final layer for 251 classes\n",
    "ssl_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ssl_model.parameters(), lr=0.001)\n",
    "\n",
    "# define a learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "lossOvertime = []\n",
    "accuracyOvertime = []\n",
    "\n",
    "ssl_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(classification_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ssl_model(images, predict_rotation=False)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    lossOvertime.append(round(running_loss / len(classification_loader), 2))\n",
    "\n",
    "    # put the model in evaluation mode\n",
    "    ssl_model.eval()\n",
    "\n",
    "    # check accuracy on val set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(classification_loaderVal, total=len(classification_loaderVal)):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = ssl_model(images, predict_rotation=False)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    accuracy = round(accuracy, 2)\n",
    "    accuracyOvertime.append(accuracy)\n",
    "    print(f\"Epoch {epoch + 1}, loss: {round(running_loss / len(classification_loader), 2)}, accuracy: {accuracy}\")\n",
    "\n",
    "    # put the model back in training mode\n",
    "    ssl_model.train()\n",
    "\n",
    "    # save the model after each epoch\n",
    "    torch.save(ssl_model.state_dict(), f\"ssl_model_classification_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "print(\"Finished Classification Training\")\n",
    "print(lossOvertime)\n",
    "print(accuracyOvertime)\n",
    "\n",
    "# plot loss and accuracy in separate graphs\n",
    "plt.plot(lossOvertime)\n",
    "plt.savefig('Media/SSL/classification_loss.png')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(accuracyOvertime)\n",
    "plt.savefig('Media/SSL/classification_accuracy.png')\n",
    "plt.close()\n",
    "\n",
    "# start testing\n",
    "print(\"Starting testing\")\n",
    "\n",
    "# put the model in evaluation mode\n",
    "ssl_model.eval()\n",
    "\n",
    "# validate the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in classification_loaderTest:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = ssl_model(images, predict_rotation=False)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Testings accuracy: {accuracy}\")"
   ],
   "id": "4ccf21269235c2ab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-11T17:26:29.971109Z",
     "start_time": "2024-06-11T17:26:29.822017Z"
    }
   },
   "source": [
    "!mkdir Data\n",
    "!mkdir Models\n",
    "!mkdir Media\n",
    "!mkdir Media/SSL\n",
    "!mkdir Models/SSL"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sottodirectory o file Data gi… esistente.\n",
      "Sottodirectory o file Models gi… esistente.\n",
      "Sottodirectory o file Media gi… esistente.\n",
      "Sintassi del comando errata.\n",
      "Sintassi del comando errata.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "150f472de24bb937"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T09:26:03.319322Z",
     "start_time": "2024-06-13T09:25:56.892426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchvision.datasets\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import itertools"
   ],
   "id": "b74e605aa6ba45b1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CNN Class",
   "id": "cec12fab87b5e8de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T09:27:18.039479Z",
     "start_time": "2024-06-13T09:27:18.008253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Network class definition\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes, size):\n",
    "        \n",
    "        # list of the convolutional layers parameters\n",
    "        convLayerNumber = 7\n",
    "        kernels = [11, 7, 7, 5, 5, 3, 3]\n",
    "        paddings = [1, 1, 1, 1, 1, 1, 1]\n",
    "        poolingsStride = [2, 0, 2, 0, 2, 0, 2]\n",
    "        poolingsKernels = [2, 0, 2, 0, 2, 0, 2]\n",
    "        filters = [8, 16, 16, 32, 64, 64, 107]\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # conv layer\n",
    "        self.conv1 = nn.Conv2d(3, filters[0], kernel_size=kernels[0], padding=paddings[0])\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=poolingsKernels[0], stride=poolingsStride[0])\n",
    "\n",
    "        self.conv2 = nn.Conv2d(filters[0], filters[1], kernel_size=kernels[1], padding=paddings[1])\n",
    "        self.conv3 = nn.Conv2d(filters[1], filters[2], kernel_size=kernels[2], padding=paddings[2])\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=poolingsKernels[2], stride=poolingsStride[2])\n",
    "\n",
    "        self.conv4 = nn.Conv2d(filters[2], filters[3], kernel_size=kernels[3], padding=paddings[3])\n",
    "        self.conv5 = nn.Conv2d(filters[3], filters[4], kernel_size=kernels[4], padding=paddings[4])\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=poolingsKernels[4], stride=poolingsStride[4])\n",
    "\n",
    "        self.conv6 = nn.Conv2d(filters[4], filters[5], kernel_size=kernels[5], padding=paddings[5])\n",
    "        self.conv7 = nn.Conv2d(filters[5], filters[6], kernel_size=kernels[6], padding=paddings[6])\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=poolingsKernels[6], stride=poolingsStride[6])\n",
    "\n",
    "        # custom code to calculate the size of the first layer\n",
    "        for i in range(convLayerNumber):\n",
    "            size = (size - kernels[i] + 2 * paddings[i]) / 1 + 1\n",
    "            if poolingsKernels[i] != 0:\n",
    "                size = int((size - poolingsKernels[i]) / poolingsStride[i] + 1)\n",
    "\n",
    "        fc1_input_size = filters[6] * size * size\n",
    "\n",
    "        print(\"First layer size: \", fc1_input_size)\n",
    "\n",
    "        # dense nn layers\n",
    "        self.fc1 = nn.Linear(fc1_input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "         # Feature extraction\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv4(x))\n",
    "        x = F.leaky_relu(self.conv5(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv6(x))\n",
    "        x = F.leaky_relu(self.conv7(x))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Classification\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "id": "ba3b450727368461",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T09:27:18.422752Z",
     "start_time": "2024-06-13T09:27:18.381492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if train_on_gpu else \"cpu\")\n",
    "print(\"running on: \", device)\n",
    "\n",
    "# MacOS specific code\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print(\"MPS device not found.\")\n",
    "\n",
    "# Image size\n",
    "size = 128\n",
    "\n",
    "# mean and std values for the dataset calculated\n",
    "mean = [0.6388, 0.5446, 0.4452]\n",
    "std = [0.2252, 0.2437, 0.2661]\n",
    "\n",
    "# network\n",
    "net = Net(num_classes=251, size=size)\n",
    "\n",
    "# torch-summary to check the network parameters\n",
    "summary(net, (3, size, size))"
   ],
   "id": "e85fff0cb71c53c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on:  cpu\n",
      "MPS device not found.\n",
      "First layer size:  2675\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 120, 120]           2,912\n",
      "         MaxPool2d-2            [-1, 8, 60, 60]               0\n",
      "            Conv2d-3           [-1, 16, 56, 56]           6,288\n",
      "            Conv2d-4           [-1, 16, 52, 52]          12,560\n",
      "         MaxPool2d-5           [-1, 16, 26, 26]               0\n",
      "            Conv2d-6           [-1, 32, 24, 24]          12,832\n",
      "            Conv2d-7           [-1, 64, 22, 22]          51,264\n",
      "         MaxPool2d-8           [-1, 64, 11, 11]               0\n",
      "            Conv2d-9           [-1, 64, 11, 11]          36,928\n",
      "           Conv2d-10          [-1, 107, 11, 11]          61,739\n",
      "        MaxPool2d-11            [-1, 107, 5, 5]               0\n",
      "           Linear-12                  [-1, 256]         685,056\n",
      "          Dropout-13                  [-1, 256]               0\n",
      "           Linear-14                  [-1, 256]          65,792\n",
      "           Linear-15                  [-1, 251]          64,507\n",
      "================================================================\n",
      "Total params: 999,878\n",
      "Trainable params: 999,878\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 2.52\n",
      "Params size (MB): 3.81\n",
      "Estimated Total Size (MB): 6.52\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:07:29.076496Z",
     "start_time": "2024-06-13T07:07:27.216610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load net to device\n",
    "net.to(device)\n",
    "\n",
    "# for kaggle use: \"/kaggle/input/supervised/\"\n",
    "datasetPath = './Data/'\n",
    "# datasetPath = '/kaggle/input/supervised/'\n",
    "simpleCNNLossOvertime = []\n",
    "simpleCNNAccuracyOvertime = []\n",
    "\n",
    "# data transformations pipeline\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((size, size), antialias = True),\n",
    "    torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# load the dataset\n",
    "trainSet = torchvision.datasets.ImageFolder(root= datasetPath + 'processedData/processed_train_set', transform=transforms)\n",
    "testSet = torchvision.datasets.ImageFolder(root= datasetPath + 'processedData/processed_test_set', transform=transforms)\n",
    "valSet = torchvision.datasets.ImageFolder(root= datasetPath + 'processedData/processed_val_set', transform=transforms)\n",
    "\n",
    "# data loaders\n",
    "trainLoader = DataLoader(trainSet, batch_size=64, shuffle=True, num_workers=4)\n",
    "testLoader = DataLoader(testSet, batch_size=64, shuffle=True, num_workers=4)\n",
    "valLoader = DataLoader(valSet, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "# print the dataset sizes\n",
    "print(\"Training:\", len(trainLoader.dataset))\n",
    "print(\"Validation:\", len(valLoader.dataset))\n",
    "print(\"Test:\", len(testLoader.dataset))"
   ],
   "id": "36d7e8b2032180f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 88977\n",
      "Validation: 29600\n",
      "Test: 47976\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T17:34:36.494548Z",
     "start_time": "2024-06-11T17:26:36.847644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# criterion for classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# number of epochs\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # tqdm to show a progress bar \n",
    "    for i, data in tqdm(enumerate(trainLoader, 0), total=len(trainLoader)):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    simpleCNNLossOvertime.append(round(running_loss/len(trainLoader), 2))\n",
    "    \n",
    "    # put the network in evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # check accuracy on val set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(valLoader, total=len(valLoader)):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    accuracy = round(accuracy, 2)\n",
    "    simpleCNNAccuracyOvertime.append(accuracy)\n",
    "    print(f\"Epoch {epoch + 1}, loss: {round(running_loss/len(trainLoader), 2)}, accuracy: {accuracy}, lr: {scheduler.get_last_lr()}\")\n",
    "\n",
    "    # save model after each epoch\n",
    "    torch.save(net.state_dict(), f\"Models/{size}-model_{epoch}.pth\")\n",
    "    \n",
    "    # put the network back in training mode\n",
    "    net.train()\n",
    "\n",
    "print('Finished Training')\n",
    "print(simpleCNNLossOvertime)\n",
    "print(simpleCNNAccuracyOvertime)\n",
    "\n",
    "# plot loss and accuracy in separate graphs\n",
    "plt.plot(simpleCNNLossOvertime)\n",
    "plt.title('Simple CNN loss')\n",
    "plt.grid()\n",
    "plt.savefig('Media/loss.png')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(simpleCNNAccuracyOvertime)\n",
    "plt.title('Simple CNN accuracy')\n",
    "plt.grid()\n",
    "plt.savefig('Media/accuracy.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Starting testing\")\n",
    "\n",
    "# put the network in evaluation mode\n",
    "net.eval()\n",
    "\n",
    "# validate the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(testLoader, total=len(testLoader)):\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true += labels.tolist()\n",
    "        y_pred += predicted.tolist()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Testings accuracy: {accuracy}\")\n",
    "\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "print(f\"F1 score: {f1}\")"
   ],
   "id": "165af3a955f18454",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 527/1391 [07:43<12:40,  1.14it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 21\u001B[0m\n\u001B[0;32m     19\u001B[0m inputs, labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     20\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 21\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m     23\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\supervised\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\supervised\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[3], line 56\u001B[0m, in \u001B[0;36mNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;66;03m# Feature extraction\u001B[39;00m\n\u001B[1;32m---> 56\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlrn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mleaky_relu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     57\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool1(x)\n\u001B[0;32m     59\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlrn(F\u001B[38;5;241m.\u001B[39mleaky_relu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x)))\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\supervised\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\supervised\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\supervised\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:58\u001B[0m, in \u001B[0;36mLocalResponseNorm.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m---> 58\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlocal_response_norm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbeta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m                                 \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mk\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\supervised\\lib\\site-packages\\torch\\nn\\functional.py:2621\u001B[0m, in \u001B[0;36mlocal_response_norm\u001B[1;34m(input, size, alpha, beta, k)\u001B[0m\n\u001B[0;32m   2619\u001B[0m     div \u001B[38;5;241m=\u001B[39m avg_pool3d(div, (size, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m), stride\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m   2620\u001B[0m     div \u001B[38;5;241m=\u001B[39m div\u001B[38;5;241m.\u001B[39mview(sizes)\n\u001B[1;32m-> 2621\u001B[0m div \u001B[38;5;241m=\u001B[39m \u001B[43mdiv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43malpha\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2622\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m/\u001B[39m div\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Self Supervised Learning Task",
   "id": "a4125b0c4c752760"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# dataset class which extends the Dataset class\n",
    "class JigsawPuzzleDataset(Dataset):\n",
    "    def __init__(self, dataset, grid_size=2):\n",
    "        self.dataset = dataset\n",
    "        self.grid_size = grid_size\n",
    "        self.permutations = self._generate_permutations(grid_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, _ = self.dataset[index]\n",
    "        image_pieces, correct_order = self._divide_image(image)\n",
    "        shuffled_pieces, shuffled_order = self._shuffle_pieces(image_pieces)\n",
    "\n",
    "        # Create a new blank image of the correct size\n",
    "        reconstructed_image = torch.zeros_like(image)\n",
    "\n",
    "        piece_w, piece_h = image.shape[1] // self.grid_size, image.shape[2] // self.grid_size\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                # Calculate the position of the piece in the reconstructed image\n",
    "                pos = (j * piece_w, i * piece_h)\n",
    "                # Paste the piece at the correct position\n",
    "                reconstructed_image[:, pos[1]:pos[1]+piece_h, pos[0]:pos[0]+piece_w] = shuffled_pieces[i * self.grid_size + j]\n",
    "\n",
    "        # One-hot encode the permutation index\n",
    "        permutation_index = self.permutations.index(tuple(shuffled_order))\n",
    "        label = torch.zeros(len(self.permutations), dtype=torch.float)\n",
    "        label[permutation_index] = 1.0\n",
    "\n",
    "        return reconstructed_image, label\n",
    "\n",
    "    def _divide_image(self, image):\n",
    "        pieces = []\n",
    "\n",
    "        piece_w, piece_h = image.shape[1] // self.grid_size, image.shape[2] // self.grid_size\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                piece = image[:, i*piece_h:(i+1)*piece_h, j*piece_w:(j+1)*piece_w]\n",
    "                pieces.append(piece)\n",
    "\n",
    "        correct_order = list(range(self.grid_size ** 2))\n",
    "        return pieces, correct_order\n",
    "\n",
    "    def _shuffle_pieces(self, pieces):\n",
    "        shuffled_order = random.choice(self.permutations)\n",
    "        shuffled_pieces = [pieces[i] for i in shuffled_order]\n",
    "        return shuffled_pieces, shuffled_order\n",
    "\n",
    "    def _generate_permutations(self, grid_size):\n",
    "        indices = list(range(grid_size ** 2))\n",
    "        permutations = list(itertools.permutations(indices))\n",
    "        return permutations"
   ],
   "id": "eef554da3dd8cd39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# define new network class, that extends the previous one\n",
    "class NetWithJigSawPrediction(Net):\n",
    "    def __init__(self, num_classes, size):\n",
    "        super(NetWithJigSawPrediction, self).__init__(num_classes, size)\n",
    "        \n",
    "        # 24 corresponds to the number of permutations of the image pieces\n",
    "        self.jigsaw_fc = nn.Linear(self.fc1.in_features, 24)\n",
    "\n",
    "    def forward(self, x, predictJigSaw=False):\n",
    "        # Feature extraction\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv4(x))\n",
    "        x = F.leaky_relu(self.conv5(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv6(x))\n",
    "        x = F.leaky_relu(self.conv7(x))\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = F.leaky_relu(self.conv8(x))\n",
    "        x = F.leaky_relu(self.conv9(x))\n",
    "        x = self.pool5(x)\n",
    "\n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        if predictJigSaw:\n",
    "            return self.jigsaw_fc(x)\n",
    "        else:\n",
    "            x = F.leaky_relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = F.leaky_relu(self.fc2(x))\n",
    "            return self.fc3(x)"
   ],
   "id": "a37c1a535e5ccfe6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# as before use the same mean and std values\n",
    "mean = [0.6388, 0.5446, 0.4452]\n",
    "std = [0.2252, 0.2437, 0.2661]\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((128, 128), antialias= True),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# load the dataset\n",
    "trainSet = torchvision.datasets.ImageFolder(root= datasetPath + 'processedData/processed_train_set', transform=transforms)\n",
    "testSet = torchvision.datasets.ImageFolder(root= datasetPath + 'processedData/processed_test_set', transform=transforms)\n",
    "valSet = torchvision.datasets.ImageFolder(root= datasetPath + 'processedData/processed_val_set', transform=transforms)\n",
    "\n",
    "jigsaw_dataset = JigsawPuzzleDataset(trainSet)\n",
    "jigsaw_datasetTest = JigsawPuzzleDataset(testSet)\n",
    "jigsaw_datasetVal = JigsawPuzzleDataset(valSet)\n",
    "\n",
    "batch_size = 64\n",
    "jigsaw_loader = DataLoader(jigsaw_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "jigsaw_loaderTest = DataLoader(jigsaw_datasetTest, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "jigsaw_loaderVal = DataLoader(jigsaw_datasetVal, batch_size=batch_size, shuffle=True, num_workers=4)"
   ],
   "id": "7937000aa758ce63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize the SSL model\n",
    "ssl_model = NetWithJigSawPrediction(num_classes=251, size=128)\n",
    "ssl_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ssl_model.parameters(), lr=0.001)\n",
    "\n",
    "# learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "SSLLossOvertime = []\n",
    "SSLAccuracyOvertime = []\n",
    "\n",
    "ssl_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(jigsaw_loader, total=len(jigsaw_loader)):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ssl_model(images, predictJigSaw=True)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    SSLLossOvertime.append(round(running_loss / len(jigsaw_loader), 2))\n",
    "\n",
    "    # put the model in evaluation mode\n",
    "    ssl_model.eval()\n",
    "\n",
    "    # check accuracy on val set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(jigsaw_loaderVal, total=len(jigsaw_loaderVal)):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = ssl_model(images, predictJigSaw=True)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    accuracy = round(accuracy, 2)\n",
    "    SSLAccuracyOvertime.append(accuracy)\n",
    "    print(f\"Epoch {epoch + 1}, loss: {round(running_loss / len(jigsaw_loader), 2)}, accuracy: {accuracy}\")\n",
    "\n",
    "    # put the model back in training mode\n",
    "    ssl_model.train()\n",
    "\n",
    "    # save the model after each epoch\n",
    "    torch.save(ssl_model.state_dict(), f\"ssl_model_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "print(\"Finished SSL Training\")\n",
    "print(SSLLossOvertime)\n",
    "print(SSLAccuracyOvertime)\n",
    "\n",
    "# plot loss and accuracy in separate graphs\n",
    "plt.plot(SSLLossOvertime)\n",
    "plt.title('SSL Loss')\n",
    "plt.grid()\n",
    "plt.savefig('Media/SSLLoss.png')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(SSLAccuracyOvertime)\n",
    "plt.title('SSL Accuracy')\n",
    "plt.grid()\n",
    "plt.savefig('Media/SSLAccuracy.png')\n",
    "plt.close()\n",
    "\n",
    "# start testing\n",
    "print(\"Starting testing\")\n",
    "\n",
    "# put the model in evaluation mode\n",
    "ssl_model.eval()\n",
    "\n",
    "# validate the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(jigsaw_loaderTest, total=len(jigsaw_loaderTest)):\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = ssl_model(images, predictJigSaw=True)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Testings accuracy: {accuracy}\")\n",
    "\n",
    "print(\"train the CNN with the new weights\")\n",
    "\n",
    "classification_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((128, 128)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "classification_dataset = torchvision.datasets.ImageFolder(root=datasetPath + \"processedData/processed_train_set\",\n",
    "                                              transform=classification_transform)\n",
    "classification_datasetTest = torchvision.datasets.ImageFolder(root=datasetPath + \"processedData/processed_test_set\",\n",
    "                                                  transform=classification_transform)\n",
    "classification_datasetVal = torchvision.datasets.ImageFolder(root=datasetPath +  \"processedData/processed_val_set\",\n",
    "                                                 transform=classification_transform)\n",
    "\n",
    "classification_loader = DataLoader(classification_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "classification_loaderTest = DataLoader(classification_datasetTest, batch_size=batch_size, shuffle=True,\n",
    "                                       num_workers=4)\n",
    "classification_loaderVal = DataLoader(classification_datasetVal, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Fine-tune the SSL model for classification\n",
    "ssl_model.fc3 = nn.Linear(ssl_model.fc3.in_features, 251)  # Update the final layer for 251 classes\n",
    "ssl_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ssl_model.parameters(), lr=0.001)\n",
    "\n",
    "# learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "SSLCNNLossOvertime = []\n",
    "SSLCNNAccuracyOvertime = []\n",
    "\n",
    "ssl_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(classification_loader, total=len(classification_loader)):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ssl_model(images, predictJigSaw=False)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    SSLCNNLossOvertime.append(round(running_loss / len(classification_loader), 2))\n",
    "\n",
    "    # put the model in evaluation mode\n",
    "    ssl_model.eval()\n",
    "\n",
    "    # check accuracy on val set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(classification_loaderVal, total=len(classification_loaderVal)):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = ssl_model(images, predictJigSaw=False)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    accuracy = round(accuracy, 2)\n",
    "    SSLCNNAccuracyOvertime.append(accuracy)\n",
    "    print(f\"Epoch {epoch + 1}, loss: {round(running_loss / len(classification_loader), 2)}, accuracy: {accuracy}, lr: {scheduler.get_last_lr()}\")\n",
    "\n",
    "    # put the model back in training mode\n",
    "    ssl_model.train()\n",
    "\n",
    "    # save the model after each epoch\n",
    "    torch.save(ssl_model.state_dict(), f\"ssl_model_classification_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "print(\"Finished Classification Training\")\n",
    "print(SSLCNNLossOvertime)\n",
    "print(SSLCNNAccuracyOvertime)\n",
    "\n",
    "# plot loss and accuracy in separate graphs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(SSLCNNLossOvertime)\n",
    "plt.title('SSL CNN Loss')\n",
    "plt.grid()\n",
    "plt.savefig('Media/classification_loss.png')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(SSLCNNAccuracyOvertime)\n",
    "plt.title('SSL CNN Accuracy')\n",
    "plt.grid()\n",
    "plt.savefig('Media/classification_accuracy.png')\n",
    "plt.close()\n",
    "\n",
    "# start testing\n",
    "print(\"Starting testing\")\n",
    "\n",
    "# put the model in evaluation mode\n",
    "ssl_model.eval()\n",
    "\n",
    "# validate the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(classification_loaderTest, total=len(classification_loaderTest)):\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = ssl_model(images, predictJigSaw=False)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        y_true += labels.tolist()\n",
    "        y_pred += predicted.tolist()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Testings accuracy: {accuracy}\")\n",
    "\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "print(f\"F1 score: {f1}\")"
   ],
   "id": "a5044ecd86edf89e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plotting compared loss and accuracies of the models\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(SSLCNNLossOvertime)\n",
    "plt.plot(simpleCNNLossOvertime)\n",
    "plt.title('CNN Loss Compared')\n",
    "plt.legend(['SSL CNN', 'CNN'])\n",
    "plt.grid()\n",
    "plt.savefig('./Media/lossCompared.png')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(SSLCNNAccuracyOvertime)\n",
    "plt.plot(simpleCNNAccuracyOvertime)\n",
    "plt.title('CNN Accuracy Compared')\n",
    "plt.legend(['SSL CNN', 'CNN'])\n",
    "plt.grid()\n",
    "plt.savefig('./Media/accuracyCompared.png')\n",
    "plt.close()"
   ],
   "id": "6c58f7c19f24b602",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

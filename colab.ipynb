{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Donwload data"
   ],
   "id": "7d414ecde9e8cb14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "id": "86b968a44eb8b118"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!mkdir Data\n",
    "!mkdir Models\n",
    "!mkdir Media"
   ],
   "id": "cea01604ad45b665"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Path to the zip file\n",
    "zip_file_path = '/content/drive/My Drive/Project/processedData.zip' # Michele's path\n",
    "\n",
    "# Path to extract the contents\n",
    "extract_path = '/content/Data'\n",
    "\n",
    "# Create the extract path directory if it doesn't exist\n",
    "!mkdir -p \"$extract_path\"\n",
    "\n",
    "# Unzip the file\n",
    "!unzip \"$zip_file_path\" -d \"$extract_path\"\n",
    "\n",
    "print('Unzipping complete!')"
   ],
   "id": "62256e5984713e05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Network"
   ],
   "id": "caf10d59c14b8439"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes, size):\n",
    "\n",
    "        # list of of the conv layers parameters\n",
    "        convLayerNumber = 4\n",
    "        kernels = [11, 5, 3, 3]\n",
    "        paddings = [1, 1, 1, 1]\n",
    "        poolingsStride = [2, 2, 2, 2]\n",
    "        poolingsKernels = [2, 2, 2, 2]\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "        # Define the feature extraction part of the network\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=kernels[0], padding=paddings[0])\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=poolingsKernels[0], stride=poolingsStride[0])\n",
    "        # 122\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=kernels[1], padding=paddings[1])\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=poolingsKernels[1], stride=poolingsStride[1])\n",
    "        # 124\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=kernels[2], padding=paddings[2])\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=poolingsKernels[2], stride=poolingsStride[2])\n",
    "        # 124\n",
    "\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=kernels[3], padding=paddings[3])\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=poolingsKernels[3], stride=poolingsStride[3])\n",
    "        # 124\n",
    "\n",
    "        # now i want to calculate the final dimension of all the conv layers\n",
    "        for i in range(convLayerNumber):\n",
    "            size = (size - kernels[i] + 2 * paddings[i]) / 1 + 1\n",
    "            size = int((size - poolingsKernels[i]) / poolingsStride[i] + 1)\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate the size of the input to the first fully connected layer\n",
    "        # Input image size is (128, 128)\n",
    "        # After first pooling: (128/2) = 64\n",
    "        # After second pooling: (64/2) = 32\n",
    "        # After third pooling: (32/2) = 16\n",
    "        # After fourth pooling: (16/2) = 8\n",
    "\n",
    "        fc1_input_size = 32 * size * size\n",
    "\n",
    "        # Define the classification part of the network\n",
    "        self.fc1 = nn.Linear(fc1_input_size, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = F.leaky_relu(self.conv4(x))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Classification\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "    def trainStep(self, x, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        out = self.forward(x)\n",
    "\n",
    "        # create a tensor for each class\n",
    "        target = torch.zeros((y.size(0), 251))\n",
    "        target[range(y.size(0)), y] = 1\n",
    "\n",
    "        loss = self.criterion(out, target)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n"
   ],
   "id": "9277e69400858dd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "id": "f06a33f9099feed3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torchvision.datasets\n",
    "from net import Net\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def main(loadPreTrained: bool):\n",
    "    train_on_gpu = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if train_on_gpu else \"cpu\")\n",
    "    print(\"running on: \", device)\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        mps_device = torch.device(\"mps\")\n",
    "        x = torch.ones(1, device=mps_device)\n",
    "        print(x)\n",
    "    else:\n",
    "        print(\"MPS device not found.\")\n",
    "\n",
    "    size = 128\n",
    "    mean = [0.5, 0.5, 0.5]\n",
    "    std = [0.5, 0.5, 0.5]\n",
    "    net = Net(num_classes=251, size=size)\n",
    "    summary(net, (3, size, size))\n",
    "\n",
    "    # load a .pth file into the model in order to start from a pre-trained model\n",
    "    if loadPreTrained:\n",
    "        net.load_state_dict(torch.load('Models/128- model_2.pth'))\n",
    "        net.eval()\n",
    "\n",
    "\n",
    "    net.to(device)\n",
    "\n",
    "    lossOvertime = []\n",
    "    accuracyOvertime = []\n",
    "\n",
    "    # Define data transformations pipeline\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Resize((size, size)),\n",
    "        torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "    trainSet = torchvision.datasets.ImageFolder(root='./Data/processedData/processed_train_set', transform=transforms)\n",
    "    testSet = torchvision.datasets.ImageFolder(root='./Data/processedData/processed_test_set', transform=transforms)\n",
    "    valSet = torchvision.datasets.ImageFolder(root='./Data/processedData/processed_val_set', transform=transforms)\n",
    "\n",
    "    trainLoader = DataLoader(trainSet, batch_size=64, shuffle=True, num_workers=2)\n",
    "    testLoader = DataLoader(testSet, batch_size=64, shuffle=True, num_workers=2)\n",
    "    valLoader = DataLoader(valSet, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "    epochs = 3\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in tqdm(enumerate(trainLoader, 0), total=len(trainLoader)):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            loss = net.trainStep(inputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        lossOvertime.append(round(running_loss/len(trainLoader), 2))\n",
    "\n",
    "        # check accuracy on val set\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(valLoader, total=len(valLoader)):\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = net(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        accuracy = round(accuracy, 2)\n",
    "        accuracyOvertime.append(accuracy)\n",
    "        print(f\"Epoch {epoch + 1}, loss: {round(running_loss/len(trainLoader), 2)}, accuracy: {accuracy}\")\n",
    "\n",
    "        title = \"\"\n",
    "        if loadPreTrained:\n",
    "            title = \"-Pre-trained model\"\n",
    "\n",
    "        # save model to drive\n",
    "        torch.save(net.state_drive(), f\"/content/drive/My Drive/Project/Models/{size}-model_{epoch}{title}.pth\")\n",
    "        # torch.save(net.state_dict(), f\"Models/{size}-model_{epoch}{title}.pth\")\n",
    "\n",
    "    print('Finished Training')\n",
    "    print(lossOvertime)\n",
    "    print(accuracyOvertime)\n",
    "\n",
    "    # plot loss and accuracy in separate graphs\n",
    "    plt.plot(lossOvertime)\n",
    "    plt.savefig('Media/loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(accuracyOvertime)\n",
    "    plt.savefig('Media/accuracy.png')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Starting testing\")\n",
    "\n",
    "    # validate the model on the test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testLoader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation accuracy: {accuracy}\")\n",
    "\n",
    "    # calculate the F1 score\n",
    "    print(\"Starting F1 score calculation\")\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data in valLoader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_true += labels.tolist()\n",
    "            y_pred += predicted.tolist()\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"F1 score: {f1}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    loadPreTrained = False\n",
    "\n",
    "    main(loadPreTrained)"
   ],
   "id": "fd760cdaada87246"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
